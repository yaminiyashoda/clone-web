<!DOCTYPE HTML>
<!--
	ZeroFour by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>


<!-- Mirrored from 2022.ieeeicme.org/special-sessions.html by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 08 Jul 2022 07:35:25 GMT -->
<head>
	<title>Special Sessions - ICME2022</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

	<!-- Favicon -->
	<link rel="icon" href="assets/favicon.ico">

	<!-- Stylesheet -->
	<link href="../cdn.jsdelivr.net/npm/bootstrap%405.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="no-sidebar is-preload">
	<div id="page-wrapper">

		<!-- Header Wrapper -->
		<div id="header-wrapper">
			<div class="container">
				<!-- Header -->
				<header id="header">
					<div class="inner">
						<!-- Nav Start -->
						<script src="assets/js/partials/navbar.js"></script>
						<!-- Nav End -->
					</div>
				</header>
			</div>
		</div>

		<!-- Main Wrapper -->
		<div id="main-wrapper">
			<div class="wrapper style2">
				<div class="inner">
					<section class="container box feature3">
						<header class="major text-center">
							<h2>Special Sessions</h2>
						</header>
						<section class="text-justify">

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-1" role="button" aria-expanded="false"
									aria-controls="ss-1">
									Human-Centered Intelligent Multimedia Understanding
								</a>

								<div id="ss-1" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Liu Zhenguang, Zhejiang University, China</li>
											<li>Ji Zhang, University of Southern Queensland, Australia</li>
											<li>Hao Huang, Wuhan University, China</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											To intelligently interact with humans, artificial intelligence is required
											to have a sound understanding upon human videos and images. In daily life
											and working scenes, intelligent systems are designed to analyse human’s
											interactions with surrounding environments. Typical visual understanding
											tasks involve human pose estimation, pedestrian tracking, action
											recognition, and motion prediction, etc. For individual human body, it is an
											emerging research direction to develop intelligent machines for
											physiological monitoring, medical image analysis and other health-caring
											tasks.
										</p>
										<p class="mb-1">
											This special session seeks innovative papers that exploit novel technologies
											and solutions from both industry and academia on highly effective and
											efficient human-centered intelligent multimedia understanding. The list of
											possible topics includes, but not limited to:
										</p>
										<ul class="mb-0">
											<li>human action recognition and motion prediction</li>
											<li>human pose estimation</li>
											<li>medical image analysis</li>
											<li>knowledge-driven video understanding</li>
											<li>causality-driven cross-media analysis</li>
											<li>cross-modal knowledge analysis</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-2" role="button" aria-expanded="false"
									aria-controls="ss-2">
									Beyond Accuracy: Responsible, Responsive, and Robust Multimedia Retrieval
								</a>

								<div id="ss-2" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Zhedong Zheng, National University of Singapore</li>
											<li>Linchao Zhu, University of Technology Sydney</li>
											<li>Liang Zheng, Australian National University</li>
											<li>Yi Yang, Zhejiang University</li>
											<li>Tat-Seng Chua, National University of Singapore</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											In this special session, we aim to bring together the latest advances in
											Responsible, Responsive, and Robust (3R) multimedia retrieval, and draw
											attention from both the academic and industrial communities. A Responsible
											system aims to protect users’ privacy and related rights; a Responsive
											system focuses on the efficient and effective feedback on million-scale
											input data; while a Robust system ensures reliability and reproducibility of
											predictions and avoids unnecessary fatal errors.
										</p>
										<p>
											<b>Responsible: </b>The privacy of user data must be respected and used in
											beneficial contexts, especially the biometric data of users such as the
											facial images. There are two potential solutions to protect user privacy.
											One is to leverage the generated data by GAN or the synthetic data by the 3D
											engine for the model training, where the model does not need knowledge of
											user data. Another solution is to harness the Federated Learning with fewer
											demands to user’s privacy. These two approaches are still under-explored.
											Through the special session, we hope to provide an avenue for the community
											to discuss the development and draw attention to a responsible multimedia
											retrieval system.
										</p>
										<p>
											<b>Responsive: </b>The increasing quantity of multimedia data also demands
											an efficient retrieval system, which can handle the million-scale input data
											in response time during user interactions. It remains unknown whether the
											learned representation by CNN, RNN and transformers is compatible with the
											traditional hashing approaches or other dimension reduction methods, like
											PCA. On the other hand, there is also the scientific question of efficient
											model design. The related techniques include automated machine learning
											(auto-ML), the model pruning for CNN, RNN and transformers, and the
											prompt-based learning to save the training or testing time.
										</p>
										<p>
											<b>Robustness: </b>It remains a great challenge to train system to deal with
											the out-of-distribution data. Two cases in recent years show that the
											systems trained via blind data-driven approaches may lead to unexpected and
											undesirable results. In 2015, one commercial photo system labels the African
											Americans as gorillas, which raises great concerns about the racial
											discrimination and biases of such human recognition systems. Since the
											system is blindly trained by the data and hard to tune, one quick fix to
											alleviate model biases at that time was to remove the class of gorillas in
											the system. Similarly, in 2018, a self-driving car hits a pedestrian due to
											the mis-classification of the pedestrian with her bike, which may not be
											seen by the system during training. If both systems could consider
											uncertainty and learn more invariant causal factors, such accidents can be
											avoided. Therefore, we also want to promote discussions on
											viewpoint-invariant representation learning, domain adaptation, and
											long-tailed recognition for multimedia retrieval.
										</p>
										<p class="mb-1">
											The list of possible topics includes, but is not limited to:
										</p>
										<ul class="mb-0">
											<li>
												Responsible:
												<ul>
													<li>Federal Learning for Multimedia Applications</li>
													<li>Synthetic Data / Generated Data for Representation Learning</li>
													<li>Interpretable Multimedia Learning and Visualization Techniques
													</li>
													<li>Human-Centered Multimedia Analysis</li>
												</ul>
											</li>
											<li>
												Responsive:
												<ul>
													<li>Model Compression / AutoML for Multimedia Retrieval</li>
													<li>New Hashing Methods for Representation Learning</li>
													<li>Prompt Learning for Multi-Domain Adaptation</li>
													<li>Multimedia Computing on Edge Devices</li>
												</ul>
											</li>
											<li>Robust: New Evaluation Metrics and Benchmarks</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-3" role="button" aria-expanded="false"
									aria-controls="ss-3">
									User Intent Understanding towards Sophisticated Multimedia Search
								</a>

								<div id="ss-3" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Xuemeng Song, Shandong University</li>
											<li>Meng Liu, Shandong Jianzhu University</li>
											<li>Yinwei Wei, National University of Singapore</li>
											<li>Xiaojun Chan, RMIT University, Australia</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											Thanks to the flourish of multimedia devices (e.g., smart mobile devices),
											recent years have witnessed the unprecedented growth of multimedia data in
											people’s daily life. For example, numerous multimedia data have emerged in
											both Internet, and accumulated in the video surveillance domain. Therefore,
											multimedia search, which aims to facilitate users finding the target
											multimedia data from a huge database, has gained increasing research
											attention. Early studies mainly allow users to give the simple keyword-based
											or image-based query. However, in the real world, the user’s intent may be
											rather complex and can be hardly expressed by some keywords or images.
											Accordingly, recent research efforts have been resorted to the multimedia
											search that enables more sophisticated queries, such as micro-facial
											expression, a long description sentence, a modification sentence plus a
											reference image, and even a multi-modal dialog. Although existing methods
											have achieved compelling progress, they still perform far from
											satisfactorily due to the challenging user intent understanding.
										</p>
										<p>
											The goal of this special session is to call for a coordinated effort to
											promote the user intent understanding towards sophisticated multimedia
											search, showcase innovative methodologies and ideas, introduce large scale
											real systems or applications, as well as propose new real-world datasets and
											discuss future directions. We solicit manuscripts in all fields that shed
											light on the user intent understanding towards sophisticated multimedia
											search.
										</p>
										<p class="mb-1">
											We believe the special session will offer a timely collection of research
											updates to benefit the researchers and practitioners working in the broad
											fields ranging from information retrieval, multimedia to machine learning.
											To this end, we solicit original research and survey papers addressing the
											topics listed below (but not limited to):
										</p>
										<ul class="mb-0">
											<li>User intent understanding towards sentence-based target moment
												localization</li>
											<li>User intent understanding towards user feedback-guided image/video
												retrieval</li>
											<li>User intent understanding towards multi-modal dialog systems</li>
											<li>User intent understanding towards interactive search scenarios</li>
											<li>Data analytics and demo systems for multimedia search;</li>
											<li>Multimedia search related datasets</li>
											<li>Emerging new applications belonging to multimedia search</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-4" role="button" aria-expanded="false"
									aria-controls="ss-4">
									Multimodal Intelligence
								</a>

								<div id="ss-4" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Yang Yang, University of Electronic Science and Technology of China</li>
											<li>Guoqing Wang, University of Electronic Science and Technology of China
											</li>
											<li>Yi Bin, University of Electronic Science and Technology of China</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											Understanding the world surrounding us involves processing information of
											multiple modalities – we see objects by eyes, feel texture by hands,
											communicate by language, and so on. In order for more general Artificial
											Intelligence (AI) to understand the world better, it needs to be able to
											interpret and reason about the multimodal messages. Representative
											researches include connecting vision and language for better media content
											understanding (e.g., cross-modal retrieval, image and video captioning,
											visual question answering etc.), fusing different physical sensors (e.g.,
											camera, LiDAR, tactile sensation, audio etc.) for better perception of the
											world by intelligent agents such as self-driving cars and robots. Despite
											the publishing of a series of papers proposing novel designs of multimodal
											understanding frameworks in top conferences (e.g., CVPR, ICCV, ECCV, NeurIPS
											and ICLR) and top journals (e.g., IEEE TPAMI and IJCV), and the emergence of
											appealing applications leveraging multimodal sensors for more complete and
											accurate perception, some core challenges still remain unsolved including:
											how to learn computer interpretable descriptions of heterogeneous data from
											multiple modalities, how to represent the process of changing data from one
											modality to another, how to identify relations between elements from two or
											more different modalities, how to fuse information from two or more
											modalities to perform a prediction task, and finally how to transfer
											knowledge between modalities and their latent representations.
										</p>
										<p>
											The goal of this special session is to encourage researchers to present high
											quality work and to facilitate effective discussions on the potential
											solutions to those challenges. Aimed at this, this special session expects
											submissions soliciting scientific and technical contributions regarding
											recent findings in theory, methodologies, and applications within the topic
											of multimodal intelligence. Position papers with feasibility studies and
											cross-modality issues with highly applicative flair are also encouraged
											therefore we expect a positive response from both academic and industrial
											communities.
										</p>
										<p class="mb-1">
											Potential Topics Include (but are not limited to):
										</p>
										<ul class="mb-0">
											<li>Multimodal learning</li>
											<li>Cross-modal learning</li>
											<li>X-supervised learning for multimodal data</li>
											<li>Multimodal data generation and sensors</li>
											<li>Cross-modal adaptation</li>
											<li>Embodied multimodal learning</li>
											<li>Multimodal transfer learning</li>
											<li>Multimodal applications (e.g., autonomous driving, robotics, etc.)</li>
											<li>Machine Learning studies of unusual modalities</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-5" role="button" aria-expanded="false"
									aria-controls="ss-5">
									Advances in Point Cloud Acquisition, Processing and Understanding
								</a>

								<div id="ss-5" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Shuyuan Zhu, University of Electronic Science and Technology of China
											</li>
											<li>Zhan Ma, Nanjing University</li>
											<li>Wei Hu, Peking University</li>
											<li>Giuseppe Valenzise, French Centre, National de la Recherche Scientifique
											</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											Recent years have witnessed the emergence of point clouds (PC) as one of the
											most promising formats to realistically represent 3D objects and scenes, by
											a set of unstructured points sparsely distributed in a 3D space. Similar to
											2D images or videos, efficient point cloud processing from acquisition to
											understanding is of great interest to enable applications such as augmented
											reality, autonomous driving, metaverse etc.
										</p>
										<p class="mb-1">
											Different from well-structured pixels in 2D images, unstructured and
											irregularly-sampled 3D points can efficiently render 3D objects, but require
											additional efforts to find effective representations for subsequent
											computation. For example, leveraging neighborhood correlations is difficult
											for loosely connected 3D points, and finding discriminative features is also
											a challenging problem for point cloud understanding. Thus, this special
											session solicits novel ideas and contributions for point cloud acquisition,
											processing and understanding, including but not limited to:
										</p>
										<ul class="mb-0">
											<li>Point cloud acquisition and rendering</li>
											<li>Point cloud compression and communication</li>
											<li>Point cloud understanding and vision tasks</li>
											<li>Point cloud quality evaluation and modeling</li>
											<li>Point cloud standards</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-6" role="button" aria-expanded="false"
									aria-controls="ss-6">
									Visual Question Answering for Fine-grained Multimedia Understanding
								</a>

								<div id="ss-6" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Yi Jin, Beijing Jiaotong University</li>
											<li>Shaohua Wan, Zhongnan University of Economics and Law</li>
											<li>Zan Gao, Qilu University of Technology</li>
											<li>Michele Nappi, University of Salerno</li>
											<li>Yu-Dong Zhang, University of Leicester</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											Visual Question Answering (VQA) is a recent hot topic which involves
											multimedia analysis, computer vision (CV), natural language processing
											(NLP), and even a broader perspective of artificial intelligence, which has
											attracted significant interest from the machine learning, CV, and NLP
											communities. A VQA system takes as inputs an image and a free, open-ended
											question in the form of natural language about the image and generates an
											answer in the form of natural language as output. Such a functionality can
											be useful in a wide range of applications, such as analysis of surveillance
											image or video footage from camera networks in smart city environments, and
											semantic analysis of large photographic archives.
										</p>
										<p class="mb-0">
											When we want a machine to answer a specific question about an image in
											natural language, we need to encode information about the content of the
											image and the meaning and intention of the question in a form that can be
											used by the machine to provide a reasonable answer. VQA relates to the broad
											context of AI technologies in multiple aspects: fine-grained recognition,
											object detection, behavior understanding, semantic scene analysis, and
											understanding of the semantics of free text. Because VQA combines notions
											from CV and NLP, a natural VQA solution can be obtained by integrating
											methodologies from these two subfields of AI. With recent developments in
											deep learning, a better understanding the high-level and fine-grained
											semantics of visual contents becomes possible. This special issue aims to
											provide a forum for scientists and engineers working in academia, industry
											and government to present their latest research findings and engineering
											experiences on VQA for Multimedia.
										</p>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-7" role="button" aria-expanded="false"
									aria-controls="ss-7">
									Multimedia and Infrastructure Innovations in Blockchain-Driven Metaverse
								</a>

								<div id="ss-7" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Wei Cai, The Chinese University of Hong Kong, Shenzhen</li>
											<li>Yonggang Wen, Nanyang Technological University, Singapore</li>
											<li>Maha Abdallah, Université Pierre & Marie Curie</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p class="mb-1">
											With the rapid development of blockchain-based decentralized applications
											and human-computer interaction (HCI) techniques, the Metaverse has attracted
											numerous attention in both industry and academia recently, especially after
											Facebook changed its name to Meta. The unprecedented focus on and investment
											in the Metaverse will speed up the development and breakthrough of related
											technologies, which will produce a series of open multimedia research
											questions on the Metaverse, including user-generated multimedia content,
											multimedia toolkits, NFT artwork, blockchain games, and visualizations for
											the ecosystem. In this special session, we would like to provide a venue for
											researchers in this area to publish their recent discoveries and outcomes in
											a timely manner. Topics of interest include but are not limited to:
										</p>
										<ul class="mb-0">
											<li>Toolkits and systems for user-generated contents in metaverse
												applications</li>
											<li>Incentive mechanism design for user-generated contents in metaverse
												applications</li>
											<li>Artificial intelligence for user-generated contents and other aspects of
												the metaverse</li>
											<li>Novel human-computer interaction/interface for metaverse systems</li>
											<li>Innovations in NFTs and crypto artwork in the metaverse</li>
											<li>Innovations in decentralized games design for the metaverse</li>
											<li>Visualization for the metaverse ecosystem</li>
											<li>Social network visualization and analysis for the metaverse ecosystem
											</li>
											<li>Emerging multimedia applications and services in the metaverse</li>
											<li>High-performance interactive multimedia infrastructure solutions to
												support metaverse</li>
											<li>Other multimedia research topics that are closely related to Metaverse
												systems</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-8" role="button" aria-expanded="false"
									aria-controls="ss-8">
									Advanced Multimedia Technology in Sports
								</a>

								<div id="ss-8" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Min-Chun Hu, National Tsing Hua University, Taiwan</li>
											<li>Hung-Kuo Chu, National Tsing Hua University, Taiwan</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											In recent years, multimedia technology has been widely used to analyze
											sports data and to aid the sports training process. Professional leagues
											such as MLB, NBA, and NHL introduced systems to track the players, analyze
											the performance of each player, summarize game highlights, and even predict
											the possibility of match-fixing. To enhance visual entertainment, broadcast
											companies have introduced multi-view synthesis and augmented reality
											technologies to interact with the audience in real-time. Moreover, with the
											advance of head-mounted display (HMD), trainers in different kinds of sports
											have started utilizing virtual reality technologies to improve the skills
											and mindset of the athletes. There is no doubt that multimedia technology
											has become indispensable to facilitate sports training and enhance game
											experience, which also brings tremendous business opportunities to the
											sports industry. In this special issue, we invite researchers from the
											domains of multimedia content analysis, sensor data analysis,
											virtual/augmented/mixed reality, and artificial intelligence to submit their
											work that can be applied in sports.
										</p>
										<p class="mb-1">
											Areas of interest for this special session include but are not limited to:
										</p>
										<ul class="mb-0">
											<li>Detection and tracking technology for sports related objects (e.g.
												player, ball, court)</li>
											<li>Sports event detection</li>
											<li>Highlight summarization</li>
											<li>Player/team performance prediction</li>
											<li>Sensor data analysis for sports training</li>
											<li>Pose and action analysis for athletes</li>
											<li>Multiview synthesis technology for sports videos</li>
											<li>Video codec technology for sports video broadcasting</li>
											<li>Virtual/augmented/mixed reality systems for sports training</li>
											<li>Simulation of realistic athletic movements</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-9" role="button" aria-expanded="false"
									aria-controls="ss-9">
									Deep Learning-Based Object Detection and Tracking for Autonomous Driving: Network
									Model Design, Learning, and Compression
								</a>

								<div id="ss-9" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Chih-Chung Hsu, National Cheng Kung University</li>
											<li>Li-Wei Kang, National Taiwan Normal University</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p class="mb-0">
											In Object detection has been important and challenging in computer vision
											area, which has been widely applied in several domains, such as autonomous
											driving, visual surveillance, human-machine interaction, and medical
											imaging. In recent, significant improvement in object detection has been
											achieved with the rapid development of deep learning. Deep learning is
											essentially beneficial by extracting high-level and complex abstractions as
											data representations relying on a hierarchical learning process. In
											realizing deep learning, supervised and unsupervised approaches for training
											deep architectures have also been empirically investigated based on the
											adoption of parallel computing facilities such as GPUs or CPU clusters.
											However, designing and training high-accuracy and low-complexity deep models
											for object detection is still challenging, especially for the applications
											of autonomous driving. This special session will focus on all aspects of
											deep learning-based object detection and tracking for autonomous driving,
											emphasizing network model design, learning, and compression. It aims to
											bring together leading researchers and practitioners working in the emerging
											area of deep learning, object detection, autonomous driving, and related
											topics with applications.
										</p>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-10" role="button" aria-expanded="false"
									aria-controls="ss-10">
									Advances and Applications in Multimodal Autonomous Driving
								</a>

								<div id="ss-10" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Changsheng Li, Beijing Institute of Technology</li>
											<li>Yinqiang Zheng, The University of Tokyo</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											Research on autonomous driving has attracted widespread attention, due to
											the rapid development of deep neural network in recent years which
											significantly improves the accuracy of the perception and prediction and
											could benefit the planning module as well. With the rapid development and
											application of autonomous driving technology, multimodal autonomous driving
											has become an important research area. In application, autonomous vehicles
											need to operate in a variety of scenes and modes, such as highway versus
											urban road, lane keeping versus lane change and go straight versus take
											turns at crossroad. Thus, it is necessary to make multiple predictions
											conditioned on different possible modals instead of letting one single
											prediction to capture all possible modals which is unrealistic due to the
											internal uncertainty of drivers/cyclists/pedestrians’ goal and motion.
											Additionally, autonomous vehicles usually perform multi-task, such as object
											detection, semantic segmentation, tracking, prediction, and decision-making.
											As single data source is not enough to satisfy all tasks and scenes
											simultaneously, autonomous vehicles generally adopt multi-sensor
											configuration to provide multi-modal data. Besides, the multi-modal
											characteristics of autonomous driving are also reflected in the evaluation
											of the uncertainty in specific tasks, such as the reliability of object
											detection and the multimodality of trajectory prediction. How to make safe
											and efficient planning with multi-modal prediction is also an important
											topic. Although the research of automatic driving has made dramatic
											advances, multimodal automatic driving still needs further efforts to ensure
											the robustness and reliability of autonomous driving system, which is also
											the trend of the application of autonomous driving technology.
										</p>
										<p class="mb-1">
											This special session focuses on multimodal autonomous driving, aims at
											seeking for exceptional submissions to deal with the key points and
											universal problems in the field of autonomous driving, especially those
											works related with deep learning methods. Topics of interest include, but
											are not limited to:
										</p>
										<ul class="mb-0">
											<li>The acquisition and labeling of multi-modal dataset</li>
											<li>The fusion of multi-sensor information</li>
											<li>Scenario understanding with multi-sensor input</li>
											<li>Advanced methods of multi-modal perception</li>
											<li>Multimodal trajectory prediction</li>
											<li>Novel criteria to evaluate the multi-modal characteristics</li>
											<li>Collision-free and consistent multi-agent prediction with multimodal
												output</li>
											<li>Planning methods under multimodal prediction output</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-11" role="button" aria-expanded="false"
									aria-controls="ss-11">
									Robust Representation Learning for Multimedia Image Understanding
								</a>

								<div id="ss-11" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Guangwei Gao, Nangjing University of Posts and Telecommunications, China
											</li>
											<li>Jing Xiao, Wuhan University, China</li>
											<li>Liang Liao, National Institute of Informatics, Japan</li>
											<li>Junjun Jiang, Harbin Institute of Technology, China</li>
											<li>Juncheng Li, The Chinese University of Hong Kong, China</li>
											<li>Shin'ichi Satoh, National Institute of Informatics, Japan</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p>
											Representation learning has always been an important research area in
											pattern recognition. A good representation of practical data is critical to
											achieve satisfactory performance. Broadly speaking, such presentation can be
											“intra- data representation” or “inter-data representation”. Intra-data
											representation focuses on extracting or refining the raw feature of data
											point itself. Representative methods range from the early-staged
											hand-crafted feature design (e.g. SIFT, LBP, HoG, etc.), to the feature
											extraction (e.g. PCA, LDA, LLE, etc.) and feature selection (e.g. sparsity-
											based and submodulariry-based methods) in the past two decades, until the
											recent deep neural networks (e.g. CNN, RNN, etc.). Inter-data representation
											characterizes the relationship between different data points or the
											structure carried out by the dataset. For example, metric learning, kernel
											learning and causality reasoning investigate the spatial or temporal
											relationship among different examples, while subspace learning, manifold
											learning and clustering discover the underlying structural property
											inherited by the dataset.
										</p>
										<p>
											Above analyses reflect that representation learning covers a wide range of
											research topics related to pattern recognition. On one hand, many new
											algorithms on representation learning are put forward every year to cater
											for the needs of processing and understanding various practical multimedia
											data. On the other hand, massive problems regarding representation learning
											still remain unsolved, especially for the big data and noisy data. Thereby,
											the objective of this special issue is to provide a stage for researchers
											all over the world to publish their latest and original results on
											representation learning.
										</p>
										<p class="mb-1">
											Topics include but are not limited to:
										</p>
										<ul class="mb-0">
											<li>Metric learning and kernel learning</li>
											<li>Probabilistic graphical models</li>
											<li>Multi-view/Multi-modal learning</li>
											<li>Applications of representation learning</li>
											<li>Robust representation and coding</li>
											<li>Deep learning</li>
											<li>Domain transfer learning</li>
											<li>Learning under low-quality media data</li>
										</ul>
									</div>
								</div>
							</div>

							<!-- special session card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ss-12" role="button" aria-expanded="false"
									aria-controls="ss-12">
									Advances in Language and Vision Research
								</a>

								<div id="ss-12" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Yi Cai, South China University of Technology</li>
											<li>Zhenguo Yang, Guangdong University of Technology</li>
											<li>Xudong Mao, Xiamen University</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Abstract</h4>
										<p class="mb-0">
											Vision-and-Language research is an interesting area at the nexus of Computer
											Vision and Natural Language Processing, and has attracted rapidly growing
											attention from both communities. The general aims of holding this session
											are to provide a forum for reporting and discussing completed research that
											involves both language and vision and to enable NLP and computer vision
											researchers to meet, exchange ideas, expertise and technology, and form new
											research partnerships. Research involving both language and vision computing
											spans a variety of disciplines and applications, and goes back a number of
											decades. In a recent scene shift, the big data era has thrown up a multitude
											of tasks in which vision and language are inherently linked. The explosive
											growth of visual and textual data, both online and in private repositories
											by diverse institutions and companies, has led to urgent requirements in
											terms of search, processing and management of digital content. A variety of
											vision and language tasks, benchmarked over large-scale human-annotated
											datasets, have driven tremendous progress in joint multimodal representation
											learning. This session will focus on some of the recently popular tasks in
											this domain such as visual captioning, visual grounding, visual question
											answering and reasoning, multi-modal dialogue, text-to-image generation,
											image-text retrieval, and multi-modal knowledge graph, multi-modal pattern
											recognition. We will invite or choose from this conference the most
											representative papers in these areas and discuss key principles that
											epitomize the core challenges & opportunities in multi-modal understanding,
											reasoning, and generation.
										</p>
									</div>
								</div>
							</div>


							<h3>Submission</h3>
							<p>Authors should prepare their manuscript according to the Guide for Authors of ICME
								available at <a href="author-info.html">Author Information and Submission
									Instructions</a>.</p>

							<h3>Important Dates</h3>
							<p>
								Special Session paper submission deadline:
								<s>December 12, 2021 [11:59 p.m. PST]</s>
								<span class="highlight-text">December 22, 2021 [11:59 p.m. PST]</span>
							</p>

							<h3>Special Session Chairs</h3>
							<ul>
								<li>Elisa Ricci (<a href="mailto:e.ricci@unitn.it">e.ricci@unitn.it</a>)</li>
								<li>Chia-Hung Yeh (<a href="mailto:chyeh@ntnu.edu.tw">chyeh@ntnu.edu.tw</a>)</li>
								<li>Jingjing Chen (<a
										href="mailto:chenjingjing@fudan.edu.cn">chenjingjing@fudan.edu.cn</a>)</li>
							</ul>
						</section>
					</section>
				</div>
			</div>

			<!-- Sponsers -->
			<hr>
			<script src="assets/js/partials/sponsers.js"></script>
		</div>

		<!-- Footer Wrapper -->
		<script src="assets/js/partials/footer.js"></script>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.dropotron.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="../cdn.jsdelivr.net/npm/bootstrap%405.1.3/dist/js/bootstrap.bundle.min.js"
		integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
		crossorigin="anonymous"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>


<!-- Mirrored from 2022.ieeeicme.org/special-sessions.html by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 08 Jul 2022 07:35:25 GMT -->
</html>