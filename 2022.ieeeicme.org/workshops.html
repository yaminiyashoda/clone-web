<!DOCTYPE HTML>
<!--
	ZeroFour by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>


<!-- Mirrored from 2022.ieeeicme.org/workshops.html by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 08 Jul 2022 07:35:20 GMT -->
<head>
	<title>Workshops - ICME2022</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

	<!-- Favicon -->
	<link rel="icon" href="assets/favicon.ico">

	<!-- Stylesheet -->
	<link href="../cdn.jsdelivr.net/npm/bootstrap%405.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="no-sidebar is-preload">
	<div id="page-wrapper">

		<!-- Header Wrapper -->
		<div id="header-wrapper">
			<div class="container">
				<!-- Header -->
				<header id="header">
					<div class="inner">
						<!-- Nav Start -->
						<script src="assets/js/partials/navbar.js"></script>
						<!-- Nav End -->
					</div>
				</header>
			</div>
		</div>

		<!-- Main Wrapper -->
		<div id="main-wrapper">
			<div class="wrapper style2">
				<div class="inner">
					<section class="container box feature3">
						<header class="major text-center">
							<h2>Workshops</h2>
						</header>
						<section class="text-justify">

							<!-- workshop card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ws-1" role="button" aria-expanded="false"
									aria-controls="ws-1">
									IEEE Workshop on Artificial Intelligence for Art Creation (AIART’22)
								</a>

								<div id="ws-1" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Luntian Mou, Beijing University of Technology, Beijing, China</li>
											<li>Feng Gao, Peking University, Beijing, China</li>
											<li>Zijin Li, China Conservatory of Music, Beijing, China</li>
											<li>Jiaying Liu, Peking University, Beijing, China</li>
											<li>Wen-Huang Cheng, National Chiao Tung University, Taiwan</li>
											<li>Ling Fan, Tezign.com; Tongji University Design Artificial Intelligence
												Lab, Shanghai, China</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Website</h4>
										<a href="https://aiart2022.github.io/"
											target="_blank">https://aiart2022.github.io</a>

										<h4 class="fw-bold border-bottom mb-2">Description</h4>
										<p>
											Artificial Intelligence (AI) has already fueled many academic fields as well
											as industries. In the area of art creation, AI has demonstrated its great
											potential and gained increasing popularity. People are greatly impressed by
											AI painting, composing, writing, and designing. And the emerging technology
											of metaverse even provide more opportunity for AI art. AI has not only
											exhibited a certain degree of creativity, but also helped in uncovering the
											principles and mechanisms of creativity and imagination from the perspective
											of neuroscience, cognitive science and psychology.
										</p>
										<p class="mb-0">
											This is the 4th AIART workshop to be held in conjunction with ICME 2022 in
											Taipei, and it aims to bring forward cutting-edge technologies and most
											recent advances in the area of AI art in terms of the enabling creation,
											analysis, understanding, and rendering technologies. The theme topic of
											AIART’21 will be Affective computing for AI art. And we plan to invite 5
											keynote speakers to present their insightful perspectives on AI Art.
										</p>
									</div>
								</div>
							</div>

							<!-- workshop card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ws-2" role="button" aria-expanded="false"
									aria-controls="ws-2">
									ICME workshop on Hyper-Realistic Multimedia for Enhanced Quality of Experience
									(HRMEQE)
								</a>

								<div id="ws-2" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Giuseppe Valenzise, CNRS, France</li>
											<li>Federica Battisti, University of Padova, Italy</li>
											<li>Homer Chen, National Taiwan University, Taiwan</li>
											<li>Søren Forchhammer, Technical University of Denmark, Denmark</li>
											<li>Mylène Farias, University of Brasilia, Brazil</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Website</h4>
										<a href="https://www.realvision-itn.eu/Events/ICME-2022-Workshop"
											target="_blank">https://www.realvision-itn.eu/Events/ICME-2022-Workshop</a>

										<h4 class="fw-bold border-bottom mb-2">Description</h4>
										<p>
											The aim of hyper-realistic media is to faithfully represent the physical
											world. The ultimate goal is to create an experience which is perceptually
											indistinguishable from a real scene. Traditional technologies can only
											capture a fraction of the audio-visual information, limiting the realism of
											the experience. Recent innovations in computers and audio-visual technology
											have made it possible to circumvent these bottlenecks in audio-visual
											systems. As a result, new multimedia signal processing areas have emerged
											such as light fields, point clouds, ultra-high definition, high frame rate,
											high dynamic range imaging and novel 3D audio and sound field technologies.
											The novel combinations of those technologies can facilitate a
											hyper-realistic media experience. Without a doubt, this will be the future
											frontier for new multimedia systems. However, several technological barriers
											and challenges need to be overcome in developing the best solutions
											perceptually.
										</p>
										<p class="mb-0">
											This third ICME workshop on Hyper-Realistic Multimedia for Enhanced Quality
											of Experience aims at bringing forward recent advances related to capturing,
											processing, and rendering technologies. The goal is to gather researchers
											with diverse and interdisciplinary backgrounds to cover the full multimedia
											signal chain, to efficiently develop truly perceptually enhanced multimedia
											systems.
										</p>
									</div>
								</div>
							</div>

							<!-- workshop card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ws-3" role="button" aria-expanded="false"
									aria-controls="ws-3">
									3D Multimedia Analytics, Search and Generation (3DMM)
								</a>

								<div id="ws-3" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Wu Liu, JD Explore Academy, Beijing, China</li>
											<li>Hao Su, University of California San Diego, USA</li>
											<li>Olga Diamanti, Institute for Geometry at TU Graz, Austria</li>
											<li>Yang Cong, Shenyang Institute of Automation, Chinese Academy of
												Sciences, China</li>
											<li>Tao Mei, JD Explore Academy, Beijing, China</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Website</h4>
										<a href="https://3dmm-icme2022.github.io/"
											target="_blank">https://3dmm-icme2022.github.io/</a>

										<h4 class="fw-bold border-bottom mb-2">Description</h4>
										<p>
											Today, ubiquitous multimedia sensors and large-scale computing
											infrastructures are producing at a rapid velocity of 3D multi-modality data,
											such as 3D point cloud acquired with LIDAR sensors, RGB-D videos record by
											Kinect cameras, meshes of varying topology, and volumetric data. 3D
											multimedia combines different content forms such as text, audio, images, and
											video with 3D information, which can perceive the world better since the
											real world is 3-dimensional instead of 2-dimensional. For example, the
											robots can manipulate objects successfully by recognizing the object via RGB
											frames and perceiving the object size via point cloud. Researchers have
											strived to push the limits of 3D multimedia search and generation in various
											applications, such as autonomous driving, robotic visual navigation, smart
											industrial manufacturing, logistics distribution, and logistics picking. The
											3D multimedia (e.g., the videos and point cloud) can also help the agents to
											grasp, move and place the packages automatically in logistics picking
											systems.
										</p>
										<p class="mb-1">
											Therefore, 3D multimedia analytics is one of the fundamental problems in
											multimedia understanding. Different from 3D vision, 3D multimedia analytics
											mainly research on how to fuse the 3D content with other media. It is a very
											challenging problem that involves multiple tasks such as human 3D mesh
											recovery and analysis, 3D shapes and scenes generation from real-world data,
											3D virtual talking head, 3D multimedia classification and retrieval, 3D
											semantic segmentation, 3D object detection and tracking, 3D multimedia scene
											understanding, and so on. Therefore, the purpose of this workshop is to:
										</p>
										<ol class="mb-1">
											<li>bring together the state-of-the-art research on 3D multimedia analysis;
											</li>
											<li>call for a coordinated effort to understand the opportunities and
												challenges emerging in 3D multimedia analysis;</li>
											<li>identify key tasks and evaluate the state-of-the-art methods; 4)
												showcase innovative methodologies and ideas;</li>
											<li>showcase innovative methodologies and ideas;</li>
											<li>introduce interesting real-world 3D multimedia analysis systems or
												applications;</li>
											<li>propose new real-world or simulated datasets and discuss future
												directions.</li>
										</ol>
										<p class="mb-0">
											We solicit original contributions in all fields of 3D multimedia analysis
											that explore the multi-modality data to generate the strong 3D data
											representation.
										</p>
									</div>
								</div>
							</div>

							<!-- workshop card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ws-4" role="button" aria-expanded="false"
									aria-controls="ws-4">
									Artificial Intelligence in Sports (AI-Sports)
								</a>

								<div id="ws-4" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Huang-Chia Shih, Yuan Ze University, Taiwan</li>
											<li>Takahiro Ogawa, Hokkaido University, Japan</li>
											<li>Rainer Lienhart, Augsburg University, Germany</li>
											<li>Jenq-Neng Hwang, University of Washington, USA</li>
											<li>Thomas B. Moeslund, Aalborg University, Denmark</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Website</h4>
										<a href="https://ai-sports22.github.io/"
											target="_blank">https://ai-sports22.github.io/</a>

										<h4 class="fw-bold border-bottom mb-2">Description</h4>
										<p>
											Sports data analytics nowadays has attracted much attention and promoted the
											sports industry. Coaches and teams are constantly searching for competitive
											sports data analytics that utilizes AI and computer vision techniques to
											understand the deeper and hidden semantics of sports. By learning detailed
											statistics, coaches can assess defensive athletic performance and develop
											improved strategies. Data-driven machine learning technique plays an
											important role in developing and improving sports in recent years. Many
											approaches have been proposed to extract semantic concepts or abstract
											attributes, such as objects (athletes and rackets), events, scene types, and
											captions, from sports videos. The spatiotemporal content and sensor data
											from sports matches in online and offline scenarios can be analyzed,
											filtered and visualized via multi-model fusion mechanism. Specific details
											and strategies can be extracted from the data to help coaches and players
											see the whole picture with clarity. Coaches and athletes are able to utilize
											these data to make better decisions for developing their teams. Nowadays,
											popular sports like football and basketball fuel the drive for technological
											advances in AI and machine learning.
										</p>
										<p class="mb-0">
											The goal of this workshop is to advance the field of research on the
											techniques of AI for sports data, develop more techniques to accurately
											evaluate and organize the data, and further strengthen the synergy between
											sports and science.
										</p>
									</div>
								</div>
							</div>

							<!-- workshop card -->
							<div class="card mb-3">
								<a class="btn btn-light link-primary text-decoration-none text-start fs-6 fw-bold w-100 px-5 py-2"
									data-bs-toggle="collapse" href="#ws-5" role="button" aria-expanded="false"
									aria-controls="ws-5">
									Big Surveillance Data Analysis and Processing (BIG-Surv)
								</a>

								<div id="ws-5" class="collapse">
									<div class="card-body px-5 py-4">
										<h4 class="fw-bold border-bottom mt-0 mb-2">Time</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Room</h4>
										<p>TBA</p>

										<h4 class="fw-bold border-bottom mb-2">Organizers</h4>
										<ul>
											<li>Weiyao Lin, Shanghai Jiao Tong University, China</li>
											<li>John See, Heriot-Watt University (Malaysia Campus), Malaysia</li>
											<li>Xiatian Zhu, Samsung AI Centre, Cambridge, UK</li>
										</ul>

										<h4 class="fw-bold border-bottom mb-2">Website</h4>
										<a href="https://bigsurv.github.io/"
											target="_blank">https://bigsurv.github.io/</a>

										<h4 class="fw-bold border-bottom mb-2">Description</h4>
										<p>
											With the rapid growth of video surveillance applications and services, the
											amount of surveillance videos has become extremely "big" which makes human
											monitoring tedious and difficult. Therefore, there exists a huge demand for
											smart surveillance techniques which can perform monitoring in an automatic
											or semi-automatic way. Firstly, with the huge amount of surveillance videos
											in storage, video analysis tasks such as event detection, action
											recognition, and video summarization are of increasing importance in
											applications including events-of-interest retrieval and abnormality
											detection. Secondly, with the fast increase of semantic data (e.g., objects'
											trajectory & bounding box) extracted by video analysis techniques, the
											semantic data have become an essential data type in surveillance systems,
											introducing new challenging topics, such as efficient semantic data
											processing and semantic data compression, to the community. Thirdly, with
											the rapid growthfrom the static centric-based processing to the dynamic
											collaborative computing and processing among distributed video processing
											nodes or cameras, new challenges such as multi-camera joint analysis, human
											re-identification, or distributed video processingare being issued in front
											of us. The requirement of these challenges is to extend the existing
											approaches or explore new feasible techniques.
										</p>
										<p class="mb-0">
											This workshop is intended to provide a forum for researchers and engineers
											to present their latest innovations and share their experiences on all
											aspects of design and implementation of new surveillance video analysis and
											processing techniques.
										</p>
									</div>
								</div>
							</div>


							<h3>Submission</h3>
							<p>Authors should prepare their manuscript according to the Guide for Authors of ICME
								available at <a href="author-info.html">Author Information and Submission
									Instructions</a>.</p>

							<h3>Important Dates</h3>
							<p>
								Workshop paper submission deadline: <s>March 12, 2022</s> <span
									class="highlight-text">March 21 - April 9, 2022 (workshop dependent)</span>
							</p>
							<h4>Corresponding Paper Submission Deadline</h4>
							<table class="default" style="width: 300px">
								<tbody>
									<tr>
										<td><a href="https://aiart2022.github.io/" target="_blank">AIART'22</a></td>
										<td>April 9, 2022</td>
									</tr>
									<tr>
										<td><a href="https://www.realvision-itn.eu/Events/ICME-2022-Workshop"
												target="_blank">HRMEQE</a></td>
										<td>March 26, 2022</td>
									</tr>
									<tr>
										<td><a href="https://3dmm-icme2022.github.io/" target="_blank">3DMM</a></td>
										<td>March 20, 2022</td>
									</tr>
									<tr>
										<td><a href="https://ai-sports22.github.io/" target="_blank">AI-Sports</a></td>
										<td>April 11, 2022</td>
									</tr>
									<tr>
										<td><a href="https://bigsurv.github.io/" target="_blank">BIG-Surv</a></td>
										<td>March 26, 2022</td>
									</tr>
								</tbody>
							</table>

							<h3>Workshop Chairs</h3>
							<ul>
								<li>Hyeran Byun (<a href="mailto:hrbyun@yonsei.ac.kr">hrbyun@yonsei.ac.kr</a>)</li>
								<li>Andrea Cavallaro (<a
										href="mailto:a.cavallaro@qmul.ac.uk">a.cavallaro@qmul.ac.uk</a>)</li>
								<li>Chaker Larabi (<a
										href="mailto:chaker.larabi@univ-poitiers.fr">chaker.larabi@univ-poitiers.fr</a>)
								</li>
							</ul>
						</section>
					</section>
				</div>
			</div>

			<!-- Sponsers -->
			<hr>
			<script src="assets/js/partials/sponsers.js"></script>
		</div>

		<!-- Footer Wrapper -->
		<script src="assets/js/partials/footer.js"></script>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.dropotron.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="../cdn.jsdelivr.net/npm/bootstrap%405.1.3/dist/js/bootstrap.bundle.min.js"
		integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous">
		</script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>


<!-- Mirrored from 2022.ieeeicme.org/workshops.html by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 08 Jul 2022 07:35:20 GMT -->
</html>